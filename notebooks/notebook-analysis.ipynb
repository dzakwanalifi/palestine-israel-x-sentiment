{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "This section is dedicated to loading the dataset, which is crucial for initial exploration and performing necessary analyses. The dataset, presumably containing cleaned Twitter data, is loaded into a pandas DataFrame for easy manipulation and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset to see the structure of the data and perform necessary analyses\n",
    "file_path = 'twitter_cleaned_6.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Here, we are renaming a column in our DataFrame for clarity and consistency. The column originally named 'text' is being renamed to 'text_clean'. This step is important for maintaining clear and understandable code, especially when dealing with multiple text columns that might have different processing stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename kolom text menjadi text_clean\n",
    "data.rename(columns={'text':'text_clean'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we transform the data from string representations of lists back into actual lists, which is crucial for text analysis. This is particularly important for columns that contain textual data in list format, which needs to be converted for further analysis or processing. After the conversion, we join these lists into a single string for each record in the 'text' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "# Convert the string representations of lists in the 'text' column back to lists\n",
    "data['text'] = data['text_clean'].apply(literal_eval)\n",
    "data['hastag'] = data['hastag'].apply(literal_eval)\n",
    "data['text_ngrams'] = data['text_ngrams'].apply(literal_eval)\n",
    "\n",
    "# Join the lists into strings\n",
    "data['text'] = data['text'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section involves converting the 'date' column to a datetime format. The conversion to datetime is essential for any time series analysis or operations that require date manipulation. It allows for more straightforward and accurate handling of dates in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "In this section, we perform exploratory data analysis (EDA) to understand the structure and trends within the dataset. This involves grouping the data by date, analyzing word frequencies, and identifying top users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by date by dt.day and count\n",
    "grouped = data.groupby(data['date'].dt.day)['text'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we segment the dataset based on a specific date (the 7th) to compare tweet activity and content before and after this date. This segmentation helps in identifying any changes in public sentiment or focus due to events happening around this date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pisahkan data sebelum tanggal 7 dan setelah tangal 7\n",
    "data_before = data[data['date'].dt.day < 7]\n",
    "data_after = data[data['date'].dt.day >= 7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped.to_csv('before after.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we analyze the level of engagement by different users by grouping the data by username and counting the number of texts. This helps in identifying the most active participants in the discussion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by username\n",
    "grouped_user = data.groupby(data['username'])['text'].count()\n",
    "grouped_user.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_user.to_csv('grouped_user.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section focuses on analyzing the frequency of words used in tweets before and after the specified date. This analysis helps in understanding the shift in discussion topics or sentiment in response to external events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Create a list of lists containing all the words for each tweet for each day before tanggal 7\n",
    "all_words_before = [word for tokens in data_before[\"text_ngrams\"] for word in tokens]\n",
    "\n",
    "# Create a list of lists containing all the words for each tweet for each day after tanggal 7\n",
    "all_words_after = [word for tokens in data_after[\"text_ngrams\"] for word in tokens]\n",
    "\n",
    "# Create a counter for the words for each day before\n",
    "counts_before = Counter(all_words_before)\n",
    "\n",
    "# Create a counter for the words for each day after\n",
    "counts_after = Counter(all_words_after)\n",
    "\n",
    "# Create a dataframe with the words and their respective counts for each day before tanggal 7\n",
    "df_counts_before = pd.DataFrame.from_dict(counts_before, orient='index')\n",
    "df_counts_before.rename(columns={0: 'counts'}, inplace=True)\n",
    "df_counts_before.sort_values(by=['counts'], ascending=False, inplace=True)\n",
    "\n",
    "# Create a dataframe with the words and their respective counts for each day after tanggal 7\n",
    "df_counts_after = pd.DataFrame.from_dict(counts_after, orient='index')\n",
    "df_counts_after.rename(columns={0: 'counts'}, inplace=True)\n",
    "df_counts_after.sort_values(by=['counts'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_counts_before.to_csv('df_counts_before.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_counts_after.to_csv('df_counts_after.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the analysis focuses on identifying the top words used across all tweets. This provides a general view of the most common themes or terms in the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists containing all the words for each tweet for each day\n",
    "words = [word for day in data['text_ngrams'] for word in day]\n",
    "\n",
    "# Count the words for each day\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Create a dataframe from the word counts\n",
    "counts = pd.DataFrame.from_dict(word_counts, orient='index', columns=['count'])\n",
    "\n",
    "counts.sort_values(by='count', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts.to_csv('words counts.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "This section is dedicated to preparing the dataset for sampling. The aim is to create stratified samples representing tweets before and after the specified date, allowing for more accurate analysis of each period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beri label before dan after\n",
    "data['stratum'] = ['before' if x < 7 else 'after' for x in data['date'].dt.day]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the appropriate sample size for each stratum ensures that our analysis is statistically significant. This step involves using a formula to determine the sample size needed from each stratum based on certain parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62.57928118393234, 364.15899393667183)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split based on strata\n",
    "stratum_1 = data[data['stratum'] == 'before']\n",
    "stratum_2 = data[data['stratum'] == 'after']\n",
    "\n",
    "# Now, we'll calculate the sample size for each stratum using the provided formula\n",
    "# Given values\n",
    "p = 0.5\n",
    "B = 0.05\n",
    "N_1 = len(stratum_1)  # Size of stratum 1\n",
    "N_2 = len(stratum_2)  # Size of stratum 2\n",
    "\n",
    "# Define the function to calculate the sample size using the provided formula\n",
    "def calculate_sample_size(N, p, B):\n",
    "    numerator = N * p * (1 - p)\n",
    "    denominator = ((N - 1) * (B**2 / 4)) + (p * (1 - p))\n",
    "    return numerator / denominator\n",
    "\n",
    "# Calculate sample sizes for each stratum\n",
    "n_1 = calculate_sample_size(N_1, p, B)\n",
    "n_2 = calculate_sample_size(N_2, p, B)\n",
    "\n",
    "(n_1, n_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step involves performing systematic sampling on each stratum. Systematic sampling is a method where samples are selected at regular intervals from a sorted list. This ensures that the sample is representative of the entire stratum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 9, 1, 11, 63, 364, 442)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round the sample sizes to the nearest whole number\n",
    "n_1 = round(n_1)\n",
    "n_2 = round(n_2)\n",
    "\n",
    "# Calculate the steps for systematic sampling for each stratum\n",
    "step_1 = N_1 // n_1\n",
    "step_2 = N_2 // n_2\n",
    "\n",
    "# Generate the starting point randomly between 0 and step-1 for each stratum\n",
    "start_1 = np.random.randint(0, step_1)\n",
    "start_2 = np.random.randint(0, step_2)\n",
    "\n",
    "# Create the indices of the systematic sample for each stratum\n",
    "indices_1 = range(start_1, N_1, step_1)\n",
    "indices_2 = range(start_2, N_2, step_2)\n",
    "\n",
    "# Take the systematic sample for each stratum\n",
    "systematic_sample_1 = stratum_1.iloc[indices_1]\n",
    "systematic_sample_2 = stratum_2.iloc[indices_2]\n",
    "\n",
    "# Combine the samples from both strata\n",
    "systematic_sample_combined = pd.concat([systematic_sample_1, systematic_sample_2])\n",
    "\n",
    "# Output the number of samples from each stratum and the combined systematic sample\n",
    "(start_1, start_2, step_1, step_2, n_1, n_2, len(systematic_sample_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # buat dataframe yang berisi 3 kolom yakni stratum, N, n\n",
    "# stratum = ['before', 'after']\n",
    "# N = [N_1, N_2]\n",
    "# n = [n_1, n_2]\n",
    "\n",
    "# df_N = pd.DataFrame({'stratum':stratum, 'N':N, 'n':n})\n",
    "\n",
    "# df_N.to_csv('sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = systematic_sample_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with VADER\n",
    "This section utilizes VADER (Valence Aware Dictionary and sEntiment Reasoner) for sentiment analysis. VADER is particularly well-suited for social media text and can effectively evaluate the sentiment of short texts. The analysis will categorize each tweet into negative, positive, or neutral sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize VADER sentiment intensity analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to calculate sentiment score for a list of words\n",
    "def calculate_sentiment(text_list):\n",
    "    # Join the list of words into a single string\n",
    "    text_str = ' '.join(text_list.strip(\"[]\").replace(\"'\", \"\").split(\", \"))\n",
    "    # Calculate sentiment score\n",
    "    return sia.polarity_scores(text_str)\n",
    "\n",
    "# Apply the function to the 'text' column to get the sentiment scores\n",
    "sentiment_scores = data['text'].apply(calculate_sentiment)\n",
    "\n",
    "# Add the sentiment scores to the dataframe\n",
    "data = pd.concat([data, sentiment_scores.apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we categorize each tweet into 'negative', 'positive', or 'neutral' based on the highest sentiment score among 'neu', 'neg', and 'pos'. This categorization helps in simplifying the sentiment analysis results for further interpretation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buat kolom sentiment into negative, positive, and neutral based on neu neg and pos.\n",
    "def sentiment_into_negative_positive_neutral(neu, neg, pos):\n",
    "    if neu > neg and neu > pos:\n",
    "        return 'neutral'\n",
    "    elif neg > neu and neg > pos:\n",
    "        return 'negative'\n",
    "    elif pos > neu and pos > neg:\n",
    "        return 'positive'\n",
    "    \n",
    "data['sentiment'] = data.apply(lambda x: sentiment_into_negative_positive_neutral(x['neu'], x['neg'], x['pos']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we filter out tweets categorized as 'neutral' to focus our analysis on more clearly positive or negative sentiments. This helps in obtaining a more distinct sentiment perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop sentiment neutral\n",
    "data = data[data['sentiment'] != 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we further simplify the sentiment analysis by categorizing tweets into 'negative' and 'positive' based on the 'compound' score. A score equal to or above 0.5 is considered 'positive', while below 0.5 is considered 'negative'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buat kolom sentiment into just negative and positive based on compound colomn\n",
    "# jika compound >= 0.5 maka positive\n",
    "# jika compound < 0.5 maka negative\n",
    "\n",
    "def sentiment_into_negative_positive(compound):\n",
    "    if compound >= 0.5:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "    \n",
    "data['sentiment_2'] = data['compound'].apply(sentiment_into_negative_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with TextBlob\n",
    "In addition to VADER, we employ TextBlob for sentiment analysis. TextBlob provides a simple API to access its methods for performing basic NLP tasks. It is particularly useful for obtaining a tweet's polarity (positive/negative sentiment) and subjectivity (objective/subjective measurement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to apply sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    # Create a TextBlob object\n",
    "    analysis = TextBlob(text)\n",
    "    # Return polarity and subjectivity\n",
    "    return analysis.sentiment.polarity, analysis.sentiment.subjectivity\n",
    "\n",
    "# Apply sentiment analysis to the 'text' column (assuming the 'text' column contains stringified lists of words)\n",
    "# We will join the list of words into full sentences for the sentiment analysis\n",
    "data['sentiment'] = data['text_clean'].apply(lambda x: ' '.join(literal_eval(x))).apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we extract the subjectivity and polarity scores from the sentiment analysis results. These scores provide insights into how subjective or objective the language in the tweets is and the overall sentiment tone (positive or negative).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ambil hanya subjectivity\n",
    "data['subjectivity'] = data['sentiment'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ambil hanya polarity\n",
    "data['polarity'] = data['sentiment'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Analysis with NRCLex\n",
    "This section focuses on emotion analysis using NRCLex. NRCLex is a tool for detecting emotions in text. It assigns emotions to text based on a lexicon and rule-based approach. The analysis will identify the predominant emotion in each tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex\n",
    "\n",
    "# # Function to perform emotion analysis using NRCLex\n",
    "# def emotion_analysis_with_nrclex(text):\n",
    "#     # Create an NRCLex object for the text\n",
    "#     emotion = NRCLex(text)\n",
    "    \n",
    "#     # Access the emotions and frequency score\n",
    "#     emotions = emotion.affect_frequencies\n",
    "    \n",
    "#     # The emotion object also has other attributes and methods that can be useful:\n",
    "#     # - emotion.raw_emotion_scores: Emotion scores before normalization.\n",
    "#     # - emotion.top_emotions: The highest scoring emotions.\n",
    "#     # - emotion.words: The words in the text associated with emotions.\n",
    "    \n",
    "#     return emotions\n",
    "\n",
    "# Define a function to apply NRCLex to each text\n",
    "def apply_nrclex(text):\n",
    "    # Instantiate NRCLex with the text\n",
    "    text_object = NRCLex(text)\n",
    "    # Get the affect frequencies\n",
    "    emotion_frequencies = text_object.affect_frequencies\n",
    "    # Find the emotion with the highest frequency\n",
    "    highest_emotion = max(emotion_frequencies, key=emotion_frequencies.get)\n",
    "    # Return a tuple of the highest emotion and its score\n",
    "    return highest_emotion\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "data['emotions'] = data['text'].apply(apply_nrclex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) for Topic Modeling\n",
    "This section is dedicated to performing topic modeling using LDA. LDA is a popular method for extracting topics from a collection of documents. It helps in discovering abstract topics within the text data, which can be instrumental in understanding the underlying themes in large text corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Menyiapkan data untuk model LDA\n",
    "documents = data['text_ngrams']\n",
    "\n",
    "# Membuat kamus (dictionary) dari teks\n",
    "dictionary = Dictionary(documents)\n",
    "\n",
    "# Membuat korpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Melakukan LDA untuk jumlah topic antara 2 dan 15\n",
    "start_topics = 2\n",
    "end_topics = 10\n",
    "\n",
    "best_coherence = -1\n",
    "best_lda_model = None\n",
    "best_num_topics = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an existing LDA model is saved, it can be loaded for further analysis or comparison. This step is useful for reusing models without needing to retrain them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load lda model\n",
    "# lda_model = LdaModel.load('lda_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Coherence Scores\n",
    "To determine the optimal number of topics for the LDA model, we evaluate the coherence score for different numbers of topics. The coherence score measures the degree of semantic similarity between high scoring words in the topic, helping to choose the number of topics that make the most sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List untuk menyimpan skor koherensi dan jumlah topik\n",
    "coherence_scores = []\n",
    "num_topics_list = list(range(start_topics, end_topics + 1))\n",
    "\n",
    "# Melakukan LDA dan mengumpulkan skor koherensi\n",
    "for num_topics in num_topics_list:\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    coherence_scores.append(coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step involves visualizing the coherence scores for different numbers of topics. A graph is plotted to illustrate how coherence scores change with the number of topics, aiding in the selection of an optimal number for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat grafik\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_topics_list, coherence_scores, marker='o')\n",
    "plt.title('Coherence Score vs Number of Topics')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Topics for a Specific Model\n",
    "After determining the optimal number of topics, the model can be used to display the top words for each topic. This step is crucial for interpreting the themes and subjects that the LDA model has uncovered in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : palestine, israel, palestinian, people, land, one, free, like, right, israeli, free_palestine, civilian\n",
      "Topic 1 : palestine, israel, people, hamas, free, israel_palestine, palestinian, peace, would, supporter, one, free_palestine\n",
      "Topic 2 : palestine, israel, hamas, people, war, terrorist, peace, israel_palestine, palestinian, like, israeli, know\n",
      "Topic 3 : palestine, israel, israel_palestine, people, get, hamas, year, attack, palestinian, jew, free, like\n"
     ]
    }
   ],
   "source": [
    "# tampilkan untuk topic 4\n",
    "# Melakukan LDA dengan 4 topik\n",
    "# lda_model = LdaModel(corpus, num_topics=4, id2word=dictionary, passes=15)\n",
    "\n",
    "# # load lda model\n",
    "# lda_model = LdaModel.load('lda_model')\n",
    "\n",
    "# Menampilkan topik\n",
    "for topic in lda_model.show_topics(num_topics=4, num_words=12, formatted=False):\n",
    "    print('Topic', topic[0], ':', ', '.join([word[0] for word in topic[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we assign the most relevant topic to each document in the dataset. This step involves creating a new column in the DataFrame that contains the topic number with the highest contribution for each document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tambahkan kolom baru yang berisi topik ke berapa\n",
    "# Membuat kolom baru untuk menampung topik using lda_model\n",
    "data['topic'] = data['text_ngrams'].apply(lambda x: sorted(lda_model[dictionary.doc2bow(x)], key=lambda tup: tup[1], reverse=True)[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
