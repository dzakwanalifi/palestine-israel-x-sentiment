{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23eaf2d",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Scraper\n",
    "This notebook sets up a Twitter scraper using the `ntscraper` package to collect tweets related to the Palestine-Israel conflict for sentiment analysis purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ntscraper` package is an unofficial Python library designed for scraping Twitter data using Nitter instances. Here's how it works:\n",
    "\n",
    "1. **Initialization**: To begin using `ntscraper`, you first import the Nitter class from the package and create an instance of the scraper. You have the option to set the log level and whether to skip checking the Nitter instances during script execution.\n",
    "\n",
    "2. **Scraping Tweets**: The primary function of `ntscraper` is to scrape tweets. You can specify the term or hashtag you want to search for, or scrape tweets from a specific user profile. The scraper allows customization of various parameters, such as the number of tweets, date range (`since` and `until`), location (`near`), language, filters to apply or exclude, and the maximum number of retries for scraping a page. The default instance used by the scraper is chosen randomly unless specified.\n",
    "\n",
    "3. **Multiprocessing**: `ntscraper` supports multiprocessing, allowing multiple terms to be scraped simultaneously, each in a different process. However, it's important to only use this feature within a `if __name__ == \"__main__\"` block to avoid errors.\n",
    "\n",
    "4. **Profile Information**: Besides tweets, the scraper can also fetch profile information of Twitter users, returning details like display name, username, number of tweets, and profile picture.\n",
    "\n",
    "5. **Handling Nitter Instances**: `ntscraper` can use a random public Nitter instance or a specific one if provided. Due to recent changes on Twitter's side, some instances might not work as expected.\n",
    "\n",
    "6. **Installation**: The package can be easily installed using pip (`pip install ntscraper`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Check if ntscraper is installed, if not, install it\n",
    "try:\n",
    "    from ntscraper import Nitter\n",
    "except ImportError:\n",
    "    !pip install ntscraper\n",
    "    from ntscraper import Nitter\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf9420",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Here we import the necessary libraries for scraping and data manipulation. If `ntscraper` is not installed, it will be installed via `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scraper = Nitter()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0a368",
   "metadata": {},
   "source": [
    "## Initialize Scraper\n",
    "We create an instance of the `Nitter` scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "terms = [\"palestine\", \"ghaza\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fd8de",
   "metadata": {},
   "source": [
    "## Define Search Terms\n",
    "We define the terms 'Palestine' and 'Ghaza' on 2-1 October 2023, namely 5 days before and 5 days after the Hamas operation. The location is also determined, namely in the area around 'US' (United States). to collect relevant tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = scraper.get_tweets(terms, mode='term', until='2023-10-11', since='2023-10-02',\n",
    "                             language='en', near='USA', number=6000)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b13d8",
   "metadata": {},
   "source": [
    "## Scrape Tweets\n",
    "Execute the scraper to fetch tweets with the specified terms, within the given date range, language, and geographical proximity to the USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw = pd.DataFrame(results[0]['tweets'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266df9ed",
   "metadata": {},
   "source": [
    "## Dataframe Creation\n",
    "Convert the raw results into a pandas DataFrame for easier manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b33d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate user information into new columns\n",
    "user_df = df_raw['user'].apply(pd.Series)\n",
    "df = pd.concat([df_raw, user_df], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e15fc4",
   "metadata": {},
   "source": [
    "## Process User Information\n",
    "Extract user information into separate columns for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675247ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate 'stats' column into new columns\n",
    "stats_df = df['stats'].apply(pd.Series)\n",
    "df = pd.concat([df, stats_df], axis=1)\n",
    "\n",
    "# Drop the original 'stats' and 'user' columns\n",
    "df = df.drop(['stats', 'user'], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed9c83",
   "metadata": {},
   "source": [
    "## Process Tweet Statistics\n",
    "Separate tweet statistics into individual columns and remove the original 'stats' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('twitter_raw.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
