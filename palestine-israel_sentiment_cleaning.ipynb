{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries for Text Processing\n",
    "Load the necessary libraries for natural language processing, including NLTK for text manipulation and TextBlob for text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure all necessary libraries and resources are installed and imported\n",
    "try:\n",
    "    from textblob import Word\n",
    "except ImportError:\n",
    "    !pip install textblob\n",
    "    from textblob import Word\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataframes\n",
    "Load multiple CSV files containing raw tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_raw_new.csv')\n",
    "df2 = pd.read_csv('twitter_raw_new_2.csv')\n",
    "df3 = pd.read_csv('twitter_raw_new_3.csv')\n",
    "df4 = pd.read_csv('twitter_raw_new_4.csv')\n",
    "df_b1 = pd.read_csv('twitter_raw_new_before_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Dataframes\n",
    "Combine multiple dataframes into a single dataframe for unified processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gabungkan semua data dengan concat\n",
    "df = pd.concat([df, df2, df3, df4, df_b1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Relevant Columns\n",
    "Focus on specific columns relevant to the sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['username', 'text', 'date', 'comments', 'retweets', 'quotes', 'likes']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Parse Dates\n",
    "Normalize date format to ensure consistency and facilitate time-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The date format in the CSV seems to include a '·' character before the time, let's remove it to aid in parsing.\n",
    "df['date'] = df['date'].str.replace('·', '')\n",
    "\n",
    "# Next, convert the 'date' column to datetime format, assuming the time is already in UTC\n",
    "df['date'] = pd.to_datetime(df['date'], utc=True, format='%b %d, %Y %I:%M %p %Z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Hashtags and Mentions\n",
    "Isolate hashtags and mentions from the tweet text for separate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat kolom baru untuk menyimpan hastag\n",
    "df['hastag'] = df['text'].str.findall(r'#.*?(?=\\s|$)')\n",
    "\n",
    "# Membuat kolom baru untuk menyimpan teks tanpa hastag dengan re sub\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'#.*?(?=\\s|$)', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat kolom baru untuk menyimpan mention\n",
    "df['mentions'] = df['text'].apply(lambda text: re.findall(r'@\\w+', text))\n",
    "\n",
    "# Membuat kolom baru untuk menyimpan teks tanpa mention\n",
    "df['text'] = df['text'].apply(lambda text: re.sub(r'@\\w+', '', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplicate Text\n",
    "Remove tweets that are exact duplicates to ensure the uniqueness of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hapus text dengan kemiripan 100%\n",
    "df = df.drop_duplicates(subset='text', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Apply Text Normalization Resources\n",
    "Read in resources to normalize contractions and slangs, and apply these to the tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_df = pd.read_csv('contractions.csv')\n",
    "slangs_df = pd.read_csv('slangs.csv', index_col=0)\n",
    "\n",
    "contractions_dict = dict(zip(contractions_df.key, contractions_df.value))\n",
    "slangs_dict = dict(zip(slangs_df.Abbr, slangs_df.Fullform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Normalize Text\n",
    "Perform a series of text cleaning steps including emoji removal, URL stripping, and lowering case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.strip()\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  \n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                               u\"\\U0001F700-\\U0001F77F\"  \n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  \n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  \n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  \n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  \n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  \n",
    "                               u\"\\U0001FB00-\\U0001FBFF\"  \n",
    "                               u\"\\U0001FC00-\\U0001FCFF\"  \n",
    "                               u\"\\U0001F004-\\U0001F0CF\"  \n",
    "                               u\"\\U0001F18E-\\U0001F251\"  \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['text'] = df['text'].apply(remove_emojis)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda text: re.sub(r'http\\S+', '', text))\n",
    "\n",
    "df['text'] = df['text'].apply(lambda text: re.sub(r'\\d+', '', text))\n",
    "\n",
    "df['text'] = df['text'].apply(lambda text: re.sub(r'[^\\w\\s]', ' ', text))\n",
    "\n",
    "df['text'] = df['text'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['text'] = df['text'].apply(lambda text: ' '.join([word for word in word_tokenize(text) if word not in stop_words]))\n",
    "\n",
    "def normalize_text(text):\n",
    "    words = text.split()\n",
    "    normalized_words = [contractions_dict.get(word, word) for word in words]\n",
    "    normalized_words = [slangs_dict.get(word, word) for word in normalized_words]\n",
    "    return ' '.join(normalized_words)\n",
    "\n",
    "df['text'] = df['text'].apply(normalize_text)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda text: ' '.join([word for word in text.split() if len(word) > 2]))\n",
    "\n",
    "df['text'] = df['text'].apply(lambda text: ' '.join([word for word in text.split() if word.isalpha()]))\n",
    "\n",
    "# df['text'] = df['text'].apply(lambda text: ' '.join([Word(word).correct() for word in text.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Lemmatize Text\n",
    "Tokenize the tweet text and lemmatize each token to reduce words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(word_tokenize)\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['text'] = df['text'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Unigrams and Bigrams\n",
    "Generate unigrams and bigrams from the text to capture single words and two-word combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_use = 'text'\n",
    "\n",
    "# Define a function to create unigrams and bigrams\n",
    "# Unigrams are the words themselves (1-gram), and bigrams will be created by joining words with an underscore\n",
    "def create_unigrams_and_bigrams(text_list):\n",
    "    unigrams = text_list\n",
    "    bigrams = ['_'.join(pair) for pair in zip(text_list, text_list[1:])]\n",
    "    return unigrams + bigrams\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['text_ngrams'] = df[column_to_use].apply(create_unigrams_and_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Empty Text Entries\n",
    "Discard any rows where the text has been reduced to an empty list after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop yang text berisi []\n",
    "df = df[df['text_ngrams'].map(len) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Cleaned Data\n",
    "The cleaned and processed data is saved to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('twitter_cleaned_6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
